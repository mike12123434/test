import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import DBSCAN, KMeans, AgglomerativeClustering
from sklearn.metrics import silhouette_score
from scipy.stats import skew
import warnings
from google import genai
from google.genai import types
from google.genai.errors import APIError
import json

# Suppress warnings
warnings.filterwarnings("ignore")

st.set_page_config(page_title="å¿«é€Ÿå®¢æˆ¶åˆ†ç¾¤ (Fast Clustering)", layout="wide", page_icon="ğŸ‘¥")

# ============================================================
# Core Logic & Caching
# ============================================================

@st.cache_data
def load_data(file):
    """Load data with caching"""
    return pd.read_csv(file)

def smart_preprocessing(data, features):
    """
    Intelligently preprocess data: Log transform if skewed, then Scale.
    """
    df_clean = data[features].dropna()
    
    transform_info = {
        'log_features': [],
        'scaler': None
    }
    
    processed_data = df_clean.copy()
    
    for feat in features:
        if pd.api.types.is_numeric_dtype(processed_data[feat]) and processed_data[feat].min() >= 0:
            if skew(processed_data[feat]) > 1:
                processed_data[feat] = np.log1p(processed_data[feat])
                transform_info['log_features'].append(feat)
    
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(processed_data)
    transform_info['scaler'] = scaler
    
    return scaled_data, transform_info, df_clean

def calculate_silhouette_sample(X, labels, sample_size=5000):
    """
    Calculate Silhouette Score on a sample to avoid memory crashes, excluding noise labels (-1).
    """
    mask = labels != -1
    unique_labels = np.unique(labels[mask])
    
    if len(unique_labels) < 2:
        return -1 
    
    X_clust = X[mask]
    labels_clust = labels[mask]
    
    if len(X_clust) > sample_size:
        indices = np.random.choice(len(X_clust), sample_size, replace=False)
        X_sample = X_clust[indices]
        labels_sample = labels_clust[indices]
        return silhouette_score(X_sample, labels_sample)
    else:
        return silhouette_score(X_clust, labels_clust)

def run_clustering_optimized(X, n_clusters_range):
    """Run clustering algorithms with Memory Safety Checks"""
    results = {}
    rows = X.shape[0]
    
    st.info(f"æ­£åœ¨åˆ†æ {rows:,} ç­†è³‡æ–™...")

    # --- 1. K-Means ---
    st.write(f"â³ æ­£åœ¨åŸ·è¡Œ K-Means (ç¯„åœ: {n_clusters_range})...")
    best_score_km = -1
    best_model_km = None
    
    for k in range(n_clusters_range[0], n_clusters_range[1] + 1):
        model = KMeans(n_clusters=k, random_state=42, n_init='auto')
        labels = model.fit_predict(X)
        score = calculate_silhouette_sample(X, labels)
        
        if score > best_score_km:
            best_score_km = score
            best_model_km = (k, labels, model)
            
    results['K-Means'] = {
        'labels': best_model_km[1],
        'score': best_score_km,
        'params': f"k={best_model_km[0]}",
        'model': best_model_km[2]
    }

    # --- MEMORY SAFETY CHECK ---
    if rows > 15000:
        st.warning(f"âš ï¸ è³‡æ–™é‡éå¤§ ({rows:,} ç­†)ã€‚å·²è‡ªå‹•è·³é 'Agglomerative' èˆ‡ 'DBSCAN' ä»¥é¿å…è¨˜æ†¶é«”å´©æ½° (Memory Error)ã€‚")
        st.caption("éšå±¤å¼åˆ†ç¾¤èˆ‡å¯†åº¦åˆ†ç¾¤åœ¨è¶…é 1.5 è¬ç­†è³‡æ–™æ™‚æ¥µè€—è³‡æºï¼Œç›®å‰åƒ…åŸ·è¡Œ K-Meansã€‚")
        return results

    # --- 2. Agglomerative ---
    st.write("â³ æ­£åœ¨åŸ·è¡Œ Agglomerative Clustering...")
    # ... (Agglomerative clustering logic remains the same) ...
    best_score_agg = -1
    best_res_agg = None
    try:
        for k in range(n_clusters_range[0], n_clusters_range[1] + 1):
            model = AgglomerativeClustering(n_clusters=k)
            labels = model.fit_predict(X)
            score = calculate_silhouette_sample(X, labels)
            
            if score > best_score_agg:
                best_score_agg = score
                best_res_agg = (k, labels)
        if best_res_agg:
            results['Agglomerative'] = {
                'labels': best_res_agg[1],
                'score': best_score_agg,
                'params': f"k={best_res_agg[0]}",
                'model': None
            }
        else:
             results['Agglomerative'] = {'labels': np.full(rows, -1), 'score': -1, 'params': 'Failed', 'model': None}
    except Exception as e:
        st.warning(f"Skipping Agglomerative due to error: {e}")


    # --- 3. DBSCAN ---
    st.write("â³ æ­£åœ¨åŸ·è¡Œ DBSCAN...")
    # ... (DBSCAN clustering logic remains the same) ...
    best_score_db = -1
    best_res_db = None
    eps_range = np.arange(0.5, 1.5, 0.5) 
    min_samples_range = [5, 10]
    try:
        for eps in eps_range:
            for ms in min_samples_range:
                labels = DBSCAN(eps=eps, min_samples=ms).fit_predict(X)
                unique_labels = set(labels)
                if -1 in unique_labels: unique_labels.remove(-1)
                
                if 1 < len(unique_labels) < 20:
                    score = calculate_silhouette_sample(X, labels)
                    if score > best_score_db:
                        best_score_db = score
                        best_res_db = (eps, ms, labels)
        
        if best_res_db:
            results['DBSCAN'] = {
                'labels': best_res_db[2],
                'score': best_score_db,
                'params': f"eps={best_res_db[0]:.1f}, min={best_res_db[1]}",
                'model': None
            }
        else:
             results['DBSCAN'] = {'labels': np.full(rows, -1), 'score': -1, 'params': 'Failed', 'model': None}
    except Exception as e:
         st.warning(f"Skipping DBSCAN due to error: {e}")

    return results

# ============================================================
# LLM Function (Modified for Structured JSON Output)
# ============================================================

def generate_cluster_descriptions(df_viz, features, overall_means, api_key=None, threshold=0.15):
    """Generates a structured JSON description for each cluster using Gemini if API key provided."""
    
    grouped_means = df_viz[df_viz['Cluster'] != '-1'].groupby('Cluster')[features].mean()
    
    descriptions = {}
    
    if api_key:
        try:
            client = genai.Client(api_key=api_key)
            
            # --- MODIFIED PROMPT & SCHEMA FOR STABILITY ---
            prompt = f"""
            ä½ æ˜¯ä¸€ä½è³‡æ·±è³‡æ–™ç§‘å­¸å®¶ï¼Œè«‹æ ¹æ“šä»¥ä¸‹å®¢æˆ¶åˆ†ç¾¤çµæœï¼Œç‚ºæ¯å€‹ç¾¤çµ„æä¾›**ç°¡æ½”**çš„æ¥­å‹™è§£è®€ã€‚
            
            è«‹**åš´æ ¼**ä»¥ JSON æ ¼å¼è¼¸å‡ºï¼Œæ ¼å¼ç‚º: {{ "cluster_ID": {{ "è¼ªå»“": "...", "ç‰¹å¾µ": "...", "ç­–ç•¥": "..." }} for each cluster }}
            
            **è§£è®€å…§å®¹è¦æ±‚ (éœ€ä½¿ç”¨ä¸­æ–‡)**:
            1. **è¼ªå»“**: æè¿°è©²ç¾¤çµ„çš„æ•´é«”å®¢æˆ¶æ€§è³ª (å¦‚ï¼šé«˜åƒ¹å€¼å®¢æˆ¶ã€ä¼‘çœ å®¢æˆ¶)ã€‚
            2. **ç‰¹å¾µ**: **ç°¡æ½”**åœ°åˆ—å‡ºèˆ‡ç¸½é«”å¹³å‡ç›¸æ¯”ï¼Œåå·®è¶…é {int(threshold*100)}% çš„é—œéµç‰¹å¾µ (ä½¿ç”¨ã€Œé«˜ã€æˆ–ã€Œä½ã€)ã€‚ä¾‹å¦‚: é«˜åˆ·å¡é‡‘é¡ï¼Œä½æ´»èºåº¦ã€‚
            3. **ç­–ç•¥**: æå‡ºä¸€å€‹ç°¡æ½”æœ‰åŠ›çš„è¡ŒéŠ·æˆ–æ¥­å‹™ç­–ç•¥å»ºè­°ã€‚

            ---
            
            **è³‡æ–™èˆ‡çµæœ**
            
            ç‰¹å¾µåˆ—è¡¨: {', '.join(features)}
            ç¸½é«”å¹³å‡ (Overall Means): {overall_means.to_dict()}
            ç¾¤çµ„å¹³å‡ (Cluster Means):
            {grouped_means.to_string()}
            """
            
            # Define the nested JSON structure
            cluster_properties = {
                "è¼ªå»“": {"type": "string", "description": "è©²ç¾¤çµ„çš„æ•´é«”å®¢æˆ¶æ€§è³ªæè¿°"},
                "ç‰¹å¾µ": {"type": "string", "description": "ç°¡æ½”çš„é—œéµç‰¹å¾µåˆ—è¡¨ (ä½¿ç”¨é«˜/ä½)"},
                "ç­–ç•¥": {"type": "string", "description": "ç°¡æ½”çš„è¡ŒéŠ·æˆ–æ¥­å‹™ç­–ç•¥å»ºè­°"},
            }
            
            response = client.models.generate_content(
                model='gemini-2.5-flash',
                contents=prompt,
                config=types.GenerateContentConfig(
                    response_mime_type="application/json",
                    response_schema={
                        "type": "object",
                        "properties": {
                            f"cluster_{id}": {"type": "object", "properties": cluster_properties} 
                            for id in grouped_means.index
                        }
                    }
                ),
            )
            
            llm_output = response.text
            # We assume the output is a dictionary of dictionaries
            llm_descriptions = json.loads(llm_output)
            
            # Reformat the result to match the expected format for fallback if needed
            for cluster_id, data in llm_descriptions.items():
                if isinstance(data, dict):
                    # Combine structured fields into a single dict for consistent handling
                    descriptions[str(cluster_id).replace('cluster_', '')] = data
                
            st.success("âœ… å·²ä½¿ç”¨ Gemini API ç”Ÿæˆçµæ§‹åŒ–æ¥­å‹™è§£è®€ã€‚")
            
        except APIError as e:
            st.warning(f"Gemini API å‘¼å«å¤±æ•—: {e}. è«‹æª¢æŸ¥æ‚¨çš„ API Key æˆ– API é¡åº¦ã€‚ä½¿ç”¨é è¨­éœæ…‹è§£è®€ã€‚")
        except Exception as e:
            st.warning(f"LLM è§£è®€ç”Ÿæˆå¤±æ•—: {e}. ä½¿ç”¨é è¨­éœæ…‹è§£è®€ã€‚")
    
    # Fallback to hardcoded (if API failed or key not provided)
    if not descriptions or not api_key:
        for cluster_id, row in grouped_means.iterrows():
            high_feats = []
            low_feats = []
            
            for feat in features:
                cluster_mean = row[feat]
                overall_mean = overall_means[feat]
                deviation = (cluster_mean - overall_mean) / overall_mean if overall_mean != 0 else 0
                
                if deviation > threshold: 
                    high_feats.append(feat)
                elif deviation < -threshold:
                    low_feats.append(feat)
            
            # Generate structured output even for fallback
            if not high_feats and not low_feats:
                descriptions[str(cluster_id)] = {
                    "è¼ªå»“": "å¹³å‡å‹å®¢æˆ¶",
                    "ç‰¹å¾µ": "èˆ‡ç¸½é«”å¹³å‡ç„¡é¡¯è‘—å·®ç•°",
                    "ç­–ç•¥": "æ¨™æº–åŒ–è¡ŒéŠ·æ´»å‹•ï¼Œç¶­æŒæ—¢æœ‰é—œä¿‚ã€‚"
                }
            else:
                desc_feats = ""
                if high_feats:
                    desc_feats += f"é«˜{'ã€'.join(high_feats)}"
                if high_feats and low_feats:
                    desc_feats += "ï¼Œ"
                if low_feats:
                    desc_feats += f"ä½{'ã€'.join(low_feats)}"
                
                descriptions[str(cluster_id)] = {
                    "è¼ªå»“": "å·®ç•°åŒ–ç‰¹å¾µå®¢æˆ¶",
                    "ç‰¹å¾µ": desc_feats,
                    "ç­–ç•¥": "é‡å°é—œéµç‰¹å¾µåˆ¶å®šå°ˆå±¬è¡ŒéŠ·æ´»å‹•ï¼Œæå‡å®¢æˆ¶åƒ¹å€¼ã€‚"
                }
    
    return descriptions, grouped_means

# ============================================================
# UI Components (Modified Display Logic)
# ============================================================

def main():
    st.title("ğŸ‘¥ å¿«é€Ÿå®¢æˆ¶åˆ†ç¾¤ç³»çµ± (Fast Clustering)")

    # 1. Sidebar: Upload & Settings
    with st.sidebar:
        st.header("1. ä¸Šå‚³è³‡æ–™")
        uploaded_file = st.file_uploader("ä¸Šå‚³ CSV æª”æ¡ˆ", type=['csv'])
        st.divider()
        st.header("2. åƒæ•¸è¨­å®š")
        min_c = st.number_input("æœ€å°ç¾¤çµ„æ•¸ (Min Clusters)", 2, 5, 2)
        max_c = st.number_input("æœ€å¤§ç¾¤çµ„æ•¸ (Max Clusters)", 6, 15, 8)
        
        gemini_api_key = st.text_input("Google Gemini API Key (é¸å¡«ï¼Œç”¨æ–¼å‹•æ…‹ LLM è§£è®€)", type="password")
        if gemini_api_key:
            st.caption("ä½¿ç”¨ Gemini API éœ€æœ‰æœ‰æ•ˆ API Keyã€‚")

    if uploaded_file:
        df = load_data(uploaded_file)
        
        st.subheader("åŸå§‹è³‡æ–™é è¦½")
        st.dataframe(df.head(10))
        
        st.subheader("é¸æ“‡åˆ†ç¾¤ç‰¹å¾µ")
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        
        if not numeric_cols:
            st.error("è³‡æ–™ä¸­æ²’æœ‰æ•¸å€¼æ¬„ä½ï¼Œç„¡æ³•é€²è¡Œåˆ†ç¾¤ã€‚")
            return

        selected_features = st.multiselect(
            "è«‹é¸æ“‡æ­£å¥½ 2 æˆ– 3 å€‹ç”¨æ–¼åˆ†ç¾¤çš„æ¬„ä½ (ä»¥ä¾¿æ–¼è¦–è¦ºåŒ–):", 
            numeric_cols
        )
        
        if len(selected_features) not in [2, 3]:
            st.warning("âš ï¸ è«‹é¸æ“‡æ­£å¥½ 2 æˆ– 3 å€‹ç‰¹å¾µé€²è¡Œåˆ†æã€‚")
        else:
            if st.button("ğŸš€ é–‹å§‹åˆ†ç¾¤é‹ç®— (Start Clustering)", type="primary"):
                with st.spinner("æ­£åœ¨è™•ç†è³‡æ–™èˆ‡é‹ç®—æ¨¡å‹..."):
                    X_scaled, transform_info, df_used = smart_preprocessing(df, selected_features)
                    overall_means = df_used[selected_features].mean()
                    results = run_clustering_optimized(X_scaled, (min_c, max_c))
                    
                    st.session_state.results = results
                    st.session_state.transform_info = transform_info
                    st.session_state.features = selected_features
                    st.session_state.X_scaled = X_scaled
                    st.session_state.df_used = df_used
                    st.session_state.overall_means = overall_means
                    st.session_state.api_key = gemini_api_key
                    st.session_state.ran = True

            # --- Display Results Section ---
            if st.session_state.get('ran', False):
                st.divider()
                st.header("ğŸ¯ åˆ†ç¾¤çµæœ")
                
                results = st.session_state.results
                
                # Compare Models Table (Unchanged)
                comp_data = []
                valid_results = {name: res for name, res in results.items() if res['score'] > -1}
                for name, res in valid_results.items():
                    labels_no_noise = res['labels'][res['labels'] != -1]
                    n_clus = len(set(labels_no_noise)) if len(labels_no_noise) > 0 else 0
                    comp_data.append({'æ¨¡å‹': name, 'è¼ªå»“ä¿‚æ•¸ (Score)': f"{res['score']:.4f}", 'åˆ†ç¾¤æ•¸é‡': n_clus, 'æœ€ä½³åƒæ•¸': res['params']})
                
                if not comp_data:
                    st.error("æ‰€æœ‰åˆ†ç¾¤æ–¹æ³•çš†å¤±æ•—ï¼Œè«‹æª¢æŸ¥è³‡æ–™å“è³ªæˆ–é¸å–çš„ç‰¹å¾µã€‚")
                    return
                
                df_comp = pd.DataFrame(comp_data).sort_values('è¼ªå»“ä¿‚æ•¸ (Score)', ascending=False)
                st.table(df_comp)
                
                best_model_name = df_comp['æ¨¡å‹'].iloc[0]
                best_res = results[best_model_name]
                st.success(f"ğŸ† æœ€ä½³æ¨¡å‹: **{best_model_name}** (Score: {best_res['score']:.4f})")
                
                df_viz = st.session_state.df_used.copy()
                df_viz['Cluster'] = best_res['labels'].astype(str)
                
                # --- Cluster Interpretation (Modified Display Logic) ---
                st.subheader("ğŸ’¡ åˆ†ç¾¤æ¥­å‹™è§£è®€ (Cluster Interpretation)")
                descriptions, grouped_means = generate_cluster_descriptions(
                    df_viz, 
                    st.session_state.features, 
                    st.session_state.overall_means,
                    api_key=st.session_state.get('api_key')
                )

                # 1. Display Mean Table (Unchanged)
                st.write("**ç¾¤çµ„å¹³å‡ç‰¹å¾µå€¼æ¯”è¼ƒ (Cluster Mean Features)**")
                display_means = grouped_means.drop(index='-1', errors='ignore').round(1)
                overall_df = pd.DataFrame([st.session_state.overall_means.round(1)], index=['Overall Mean'])
                st.dataframe(pd.concat([display_means, overall_df]))
                
                # 2. Display Descriptions in Simple List Format (New Logic)
                st.markdown("## ğŸ“‹ ç°¡æ½”è§£è®€æ¸…å–®")
                st.markdown("---") 

                for cluster_id, structured_desc in descriptions.items():
                    if cluster_id == '-1':
                        continue
                    
                    # ä½¿ç”¨ .get() ç¢ºä¿å³ä½¿ JSON çµæ§‹ä¸å®Œæ•´ä¹Ÿä¸æœƒå´©æ½°
                    profile_display = structured_desc.get("è¼ªå»“", "å®¢æˆ¶è¼ªå»“æœªçŸ¥")
                    core_features = structured_desc.get("ç‰¹å¾µ", "èˆ‡ç¸½é«”å¹³å‡ç„¡é¡¯è‘—å·®ç•°")
                    strategy = structured_desc.get("ç­–ç•¥", "ç„¡æ˜ç¢ºç­–ç•¥å»ºè­°")
                    
                    st.markdown(f"### ğŸ“ Cluster {cluster_id}")
                    st.markdown(f"""
                    * **å®¢æˆ¶è¼ªå»“**: {profile_display}
                    * **æ ¸å¿ƒç‰¹å¾µ**: {core_features}
                    * **å»ºè­°ç­–ç•¥**: {strategy}
                    """)
                    st.markdown("---")

                # Visualization (Unchanged)
                # ... (Visualization logic remains the same) ...
                col1, col2 = st.columns([3, 1])
                with col1:
                    df_viz_clustered = df_viz[df_viz['Cluster'] != '-1']
                    
                    if len(selected_features) == 2:
                        fig = px.scatter(df_viz_clustered, x=selected_features[0], y=selected_features[1], color='Cluster', title=f"åˆ†ç¾¤è¦–è¦ºåŒ– ({best_model_name}) - 2D Scatter", height=500)
                        st.plotly_chart(fig, use_container_width=True)
                    elif len(selected_features) == 3:
                        fig = px.scatter_3d(df_viz_clustered, x=selected_features[0], y=selected_features[1], z=selected_features[2], color='Cluster', title=f"åˆ†ç¾¤è¦–è¦ºåŒ– ({best_model_name}) - 3D Scatter", height=500)
                        st.plotly_chart(fig, use_container_width=True)
                
                with col2:
                    st.write("**ç¾¤çµ„å¤§å°çµ±è¨ˆ (Count)**")
                    stats = df_viz_clustered['Cluster'].value_counts().reset_index()
                    stats.columns = ['Cluster', 'Count']
                    st.dataframe(stats, use_container_width=True)

                # Prediction Section (Unchanged)
                # ... (Prediction logic remains the same) ...
                st.divider()
                st.header("ğŸ” å–®ç­†é æ¸¬")
                with st.expander("è¼¸å…¥æ•¸å€¼é€²è¡Œé æ¸¬", expanded=False):
                    features = st.session_state.features
                    transform_info = st.session_state.transform_info
                    
                    inputs = {}
                    cols = st.columns(len(features))
                    for i, feat in enumerate(features):
                        with cols[i]:
                            inputs[feat] = st.number_input(f"{feat}", value=float(st.session_state.df_used[feat].mean()))
                    
                    if st.button("é æ¸¬æ‰€å±¬ç¾¤çµ„"):
                        input_df = pd.DataFrame([inputs])
                        for feat in transform_info['log_features']:
                            input_df[feat] = np.log1p(input_df[feat])
                        input_scaled = transform_info['scaler'].transform(input_df)
                        
                        pred_label = -1
                        if best_model_name == 'K-Means' and best_res['model'] is not None:
                            pred_label = best_res['model'].predict(input_scaled)[0]
                        else:
                            df_temp = pd.DataFrame(st.session_state.X_scaled)
                            df_temp['label'] = best_res['labels']
                            active_clusters = df_temp[df_temp['label'] != -1]
                            if not active_clusters.empty:
                                centroids = active_clusters.groupby('label')[list(range(len(features)))].mean().values
                                unique_labels = sorted(active_clusters['label'].unique())
                                dists = np.linalg.norm(centroids - input_scaled, axis=1)
                                pred_label = unique_labels[dists.argmin()]
                        
                        if pred_label != -1:
                            st.success(f"### è©²å®¢æˆ¶å±¬æ–¼: Cluster {pred_label}")
                            # Show structured interpretation of the predicted cluster
                            if str(pred_label) in descriptions:
                                pred_desc = descriptions[str(pred_label)]
                                st.markdown(f"""
                                **ç¾¤çµ„ {pred_label} è¼ªå»“**: 
                                * **ç‰¹å¾µ**: {pred_desc.get('ç‰¹å¾µ', 'N/A')}
                                * **ç­–ç•¥**: {pred_desc.get('ç­–ç•¥', 'N/A')}
                                """)
                        else:
                            st.warning("ç„¡æ³•å°‡æ­¤å®¢æˆ¶åˆ†é¡è‡³ä»»ä½•æœ‰æ•ˆç¾¤çµ„ã€‚")

if __name__ == "__main__":
    if 'ran' not in st.session_state:
        st.session_state.ran = False
        
    main()